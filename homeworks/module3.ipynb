{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30819bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import minsearch\n",
    "import numpy as np\n",
    "from minsearch import VectorSearch\n",
    "from rouge import Rouge\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from llmzmcp.data import (load_eval_documents, load_ground_truth_questions,\n",
    "                          load_llm_eval_dataframes)\n",
    "from llmzmcp.module3 import (evaluate_search, minsearch_query,\n",
    "                             minsearch_vector_query, qdrant_vector_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf231137",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_eval_documents()\n",
    "ground_truth = load_ground_truth_questions().to_dict(orient=\"records\")\n",
    "df_gpt4o_mini = load_llm_eval_dataframes(\"gpt4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3985ae77",
   "metadata": {},
   "source": [
    "### Q1. Minsearch text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06af7ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4627/4627 [00:04<00:00, 945.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hitrate: 0.85\n"
     ]
    }
   ],
   "source": [
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\", \"id\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)\n",
    "\n",
    "ms_res = evaluate_search(ground_truth, \n",
    "                         lambda q: minsearch_query(index, q['question'], q['course'], \n",
    "                                                   boost = {'question': 1.5, 'section': 0.1}))\n",
    "\n",
    "print(f\"Hitrate: {ms_res['hit_rate']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74695d6",
   "metadata": {},
   "source": [
    "### Q2. Vector search for question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a408753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4627/4627 [00:01<00:00, 2887.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "\n",
    "for doc in documents:\n",
    "    t = doc['question']\n",
    "    texts.append(t)\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3),\n",
    "    TruncatedSVD(n_components=128, random_state=1)\n",
    ")\n",
    "X = pipeline.fit_transform(texts)\n",
    "\n",
    "# Convert the ground_truth into vectors for evaluation\n",
    "for idx,doc in enumerate(ground_truth):\n",
    "    doc[\"vector_q\"] = pipeline.transform([doc[\"question\"]])\n",
    "    ground_truth[idx] = doc\n",
    "\n",
    "vindex = VectorSearch(keyword_fields={'course'})\n",
    "vindex.fit(X, documents)\n",
    "\n",
    "msv_res = evaluate_search(ground_truth, \n",
    "                          lambda q: minsearch_vector_query(vindex, q['vector_q'], q['course']))\n",
    "print(f\"MRR: {msv_res['mrr']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ae6859",
   "metadata": {},
   "source": [
    "### Q3. Vector search for question and answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fdf9262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4627/4627 [00:01<00:00, 2917.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hitrate: 0.822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "\n",
    "for doc in documents:\n",
    "    t = doc['question'] + ' ' + doc['text']\n",
    "    texts.append(t)\n",
    "\n",
    "X = pipeline.fit_transform(texts)\n",
    "\n",
    "# Convert the ground_truth into vectors for evaluation\n",
    "for idx,doc in enumerate(ground_truth):\n",
    "    doc[\"vector_q\"] = pipeline.transform([doc[\"question\"]])\n",
    "    ground_truth[idx] = doc\n",
    "\n",
    "vindex = VectorSearch(keyword_fields={'course'})\n",
    "vindex.fit(X, documents)\n",
    "\n",
    "msv_res = evaluate_search(ground_truth, \n",
    "                          lambda q: minsearch_vector_query(vindex, q['vector_q'], q['course']))\n",
    "print(f\"Hitrate: {msv_res['hit_rate']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c9517f",
   "metadata": {},
   "source": [
    "### Q4. Qdrant\n",
    "Now let's evaluate the following settings in Qdrant:\n",
    "\n",
    "- text = doc['question'] + ' ' + doc['text']\n",
    "- model_handle = \"jinaai/jina-embeddings-v2-small-en\"\n",
    "- limit = 5\n",
    "\n",
    "What's the MRR?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "353904a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4627/4627 [00:47<00:00, 97.99it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_handle = \"jinaai/jina-embeddings-v2-small-en\"\n",
    "collection_name = \"zoomcamp-faq\"\n",
    "\n",
    "qdrt_res = evaluate_search(ground_truth, \n",
    "                          lambda q: qdrant_vector_query(q['question'], collection_name, \n",
    "                                                        model_handle, q['course']))\n",
    "print(f\"MRR: {qdrt_res['mrr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7297062d",
   "metadata": {},
   "source": [
    "### Q5. Cosine simiarity\n",
    "Compare the answer generated by our system (`gpt-4o-mini`) with the actual answer from the FAQ using cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3f72453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cosine similarity: 0.84\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    u_norm = np.sqrt(u.dot(u))\n",
    "    v_norm = np.sqrt(v.dot(v))\n",
    "    return u.dot(v) / (u_norm * v_norm)\n",
    "\n",
    "\n",
    "text = df_gpt4o_mini.answer_llm + ' ' + df_gpt4o_mini.answer_orig + ' ' + df_gpt4o_mini.question\n",
    "rag_pipe = pipeline.fit(text)\n",
    "\n",
    "cos_sim_list = []\n",
    "for idx,rows in df_gpt4o_mini.iterrows():\n",
    "    v_llm = rag_pipe.transform([rows[\"answer_llm\"]]).flatten()\n",
    "    v_orig = rag_pipe.transform([rows[\"answer_orig\"]]).flatten()\n",
    "    cos_sim = cosine_similarity(v_llm,v_orig)\n",
    "    cos_sim_list.append(cos_sim)\n",
    "\n",
    "\n",
    "print(f\"Mean cosine similarity: {np.around(np.mean(cos_sim_list),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e635e744",
   "metadata": {},
   "source": [
    "### Q6. Rouge\n",
    "\n",
    "And alternative way to see how two texts are similar is ROUGE. <br>\n",
    "\n",
    "This is a set of metrics that compares two answers based on the overlap of n-grams, word sequences, and word pairs. <br>\n",
    "\n",
    "It can give a more nuanced view of text similarity than just cosine similarity alone. <br>\n",
    "\n",
    "There are three scores: `rouge-1`, `rouge-2` and `rouge-l`, and _precision_, _recall_ and _F1 score_ for each.\n",
    "- `rouge-1` - the overlap of unigrams,\n",
    "- `rouge-2` - bigrams,\n",
    "- `rouge-l` - the longest common subsequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3afe5f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rouge-1 F1: 0.35\n"
     ]
    }
   ],
   "source": [
    "rouge_scorer = Rouge()\n",
    "\n",
    "scores = rouge_scorer.get_scores(df_gpt4o_mini.answer_llm, df_gpt4o_mini.answer_orig)#[0]\n",
    "precision = [score[\"rouge-1\"][\"f\"] for score in scores]\n",
    "\n",
    "print(f\"Mean Rouge-1 F1: {np.around(np.mean(precision),2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmzmcp-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
